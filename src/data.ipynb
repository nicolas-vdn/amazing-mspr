{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "41a69519ea586351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_to_load = \"Data/2019-Oct.csv\"\n",
    "\n",
    "chunksize = 50_000\n",
    "fraction = 0.001\n",
    "seed = 42"
   ],
   "id": "24cdc7ddc2154882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks = pd.read_csv(file_to_load, chunksize=chunksize, parse_dates=['event_time'])",
   "id": "84f93d5938efc2fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def getRandomDataset(chunks, frac, seed):\n",
    "    return pd.concat(chunk.sample(frac=frac, random_state=seed) for chunk in chunks)"
   ],
   "id": "75a331ecec8015ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = getRandomDataset(chunks, fraction, seed)",
   "id": "e8c1edcdb6377bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploration",
   "id": "e5589d24b76888a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "cba25c85ccdf9b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.category_id.nunique()",
   "id": "880f913dfa0bee55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "b0a2c34bd97fbbeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "6bd15ac4b1546449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def createNaAndUniqueMatrix(df):\n",
    "    rows = [\"Valeurs nulles\", \"Valeurs uniques\"]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        na = df[column].isna()\n",
    "        unique = len(df[column][~na].unique())\n",
    "\n",
    "        data[column] = [f'{len(df[na])} ({(len(df[na])/len(df))*100:.2f}%)', unique]\n",
    "\n",
    "    return pd.DataFrame(data, index=rows)"
   ],
   "id": "f65ecc05ce823974",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "createNaAndUniqueMatrix(df)",
   "id": "db39974bc85ec541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analysons les 3 valeurs uniques d'event_type",
   "id": "1bb3000ce84b492"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frequences = df[\"event_type\"][df[\"event_type\"].notna()].value_counts()\n",
    "\n",
    "ax = frequences.plot(kind=\"bar\")\n",
    "\n",
    "for i,v in enumerate(frequences):\n",
    "    ax.text(i, v + 0.1, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Fréquence des types d'événements\")\n",
    "plt.xlabel(\"Événement\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()"
   ],
   "id": "ffa8fc27cd7c71d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On recense donc 407 757 vues, 9237 mises dans un panier et 7494 achats\n",
    "\n",
    "Quels sont les taux de conversion entre les types d'événements ?"
   ],
   "id": "7627a42132776864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "viewToCart = frequences[\"cart\"] / frequences[\"view\"] * 100\n",
    "cartToPurchase = frequences[\"purchase\"] / frequences[\"cart\"] * 100\n",
    "viewToPurchase = frequences[\"purchase\"] / frequences[\"view\"] * 100\n",
    "\n",
    "print(f\"Taux de conversion vue => panier : {viewToCart:.2f} %\")\n",
    "print(f\"Taux de conversion panier => achat : {cartToPurchase:.2f} %\")\n",
    "print(f\"Taux de conversion vue => achat : {viewToPurchase:.2f} %\")"
   ],
   "id": "c3e9a3a693ea30e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combien d'utilisateurs différents ont fait un achat ?",
   "id": "8d7252046d38c977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "usersPurchased = len(df[\"user_id\"][df[\"event_type\"] == \"purchase\"].unique())\n",
    "\n",
    "itemsPurchasedPerUser = frequences[\"purchase\"] / usersPurchased\n",
    "\n",
    "maxUserBuy = df[df[\"event_type\"] == \"purchase\"].groupby(\"user_id\").size().max()\n",
    "maxSessionBuy = df[df[\"event_type\"] == \"purchase\"].groupby(\"user_session\").size().max()\n",
    "\n",
    "print(f\"Un total de {usersPurchased} utilisateurs différents ont effectué des achats sur le site\")\n",
    "print(f\"Un utilisateur qui achète sur le site achète en moyenne {itemsPurchasedPerUser:.2f} articles\")\n",
    "print(f\"L'utilisateur qui a le plus acheté a acheté {maxUserBuy} articles sur le mois\")\n",
    "print(f\"La session utilisateur qui a le plus acheté a acheté {maxSessionBuy} articles en une session\")"
   ],
   "id": "85b36fc95f58b598",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comment évoluent les achats sur le mois ?",
   "id": "680af885a9d413e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "datesDayToDay = pd.to_datetime(df[\"event_time\"]).dt.date\n",
    "purchasesDayToDay = df[df[\"event_type\"] == \"purchase\"].groupby(datesDayToDay)[\"event_type\"].count()\n",
    "uniqueDates = datesDayToDay.unique()"
   ],
   "id": "2fac3a435a0283b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(uniqueDates, purchasesDayToDay)\n",
    "\n",
    "for x,y in zip(uniqueDates, purchasesDayToDay):\n",
    "    plt.text(x, y + 0.3, y, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.title(\"Ventes effectués au cours du mois\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Quantité\")\n",
    "plt.grid(True)\n",
    "plt.xticks(uniqueDates, rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "id": "54deb9d78a046643",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quel est le chiffre d'affaire quotidien ?",
   "id": "83ac66b308c86922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datesDayToDay = pd.to_datetime(df[\"event_time\"]).dt.date\n",
    "gainsDayToDay = df[df[\"event_type\"] == \"purchase\"].groupby(datesDayToDay)[\"price\"].sum()\n",
    "uniqueDates = datesDayToDay.unique()"
   ],
   "id": "1cdabc39bdf745f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.bar(uniqueDates, gainsDayToDay)\n",
    "for x,y in zip(uniqueDates, gainsDayToDay):\n",
    "    plt.text(x, y + 0.3, y, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.title(\"Chiffre d'affaire effectué au cours du mois\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Chiffre d'affaire\")\n",
    "plt.grid(True)\n",
    "plt.xticks(uniqueDates, rotation=45)\n",
    "plt.show()"
   ],
   "id": "f8b65d91c4b6282f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformation",
   "id": "e489f37677eed5f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Conversion du timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['event_time'])\n",
    "\n",
    "# 2. Extraction des composantes temporelles\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day'] = df['timestamp'].dt.dayofweek  # 0 = lundi, 6 = dimanche\n",
    "\n",
    "# 3. Catégorisation des moments de la journée\n",
    "def time_of_day(h):\n",
    "    if 6 <= h < 12:  return 'morning'\n",
    "    elif 12 <= h < 18: return 'afternoon'\n",
    "    elif 18 <= h < 24: return 'evening'\n",
    "    else: return 'night'\n",
    "\n",
    "df['time_period'] = df['hour'].apply(time_of_day)\n",
    "\n",
    "# 4. Encodage one-hot des types d’événements (sécurisé pour les colonnes manquantes)\n",
    "df = pd.get_dummies(df, columns=['event_type'], prefix='event_type')\n",
    "for col in ['event_type_view', 'event_type_cart', 'event_type_purchase']:\n",
    "    if col not in df:\n",
    "        df[col] = 0\n",
    "\n",
    "# 5. Agrégats principaux\n",
    "agg_behaviour = df.groupby('user_id').agg(\n",
    "    total_views=('event_type_view', 'sum'),\n",
    "    total_cart=('event_type_cart', 'sum'),\n",
    "    total_purchase=('event_type_purchase', 'sum'),\n",
    "    unique_categories=('category_id', 'nunique'),\n",
    "    last_activity=('timestamp', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# 6. Total dépensé uniquement sur les achats\n",
    "total_spent = (\n",
    "    df[df['event_type_purchase'] == 1]\n",
    "    .groupby('user_id')['price']\n",
    "    .sum()\n",
    "    .rename('total_spent')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_behaviour = agg_behaviour.merge(total_spent, on='user_id', how='left')\n",
    "agg_behaviour['total_spent'] = agg_behaviour['total_spent'].fillna(0)\n",
    "\n",
    "# 7. KPIs dérivés\n",
    "agg_behaviour['avg_basket'] = agg_behaviour['total_spent'] / agg_behaviour['total_purchase'].replace(0, np.nan)\n",
    "agg_behaviour['conversion_rate'] = agg_behaviour['total_purchase'] / agg_behaviour['total_views'].replace(0, np.nan)\n",
    "\n",
    "# 8. Répartition par moments de la journée (%)\n",
    "time_dist = (\n",
    "    df.groupby(['user_id', 'time_period'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .pipe(lambda d: d.div(d.sum(axis=1), axis=0))\n",
    ")\n",
    "\n",
    "# 9. Heure moyenne d’activité (cyclique)\n",
    "df['hour_rad'] = 2 * np.pi * df['hour'] / 24\n",
    "hour_avg = df.groupby('user_id').agg(\n",
    "    hour_cos=('hour_rad', lambda x: np.mean(np.cos(x))),\n",
    "    hour_sin=('hour_rad', lambda x: np.mean(np.sin(x)))\n",
    ").reset_index()\n",
    "hour_avg['peak_hour'] = np.arctan2(hour_avg['hour_sin'], hour_avg['hour_cos']) * (24 / (2 * np.pi))\n",
    "hour_avg['peak_hour'] = hour_avg['peak_hour'] % 24\n",
    "\n",
    "# 10. Récence en jours\n",
    "now = df['timestamp'].max()\n",
    "agg_behaviour['recency_days'] = (now - agg_behaviour['last_activity']).dt.days\n",
    "\n",
    "# 11. Fusion finale + nettoyage\n",
    "df_user_features = (\n",
    "    agg_behaviour\n",
    "    .merge(time_dist, on='user_id', how='left')\n",
    "    .merge(hour_avg[['user_id', 'peak_hour']], on='user_id', how='left')\n",
    "    .drop(columns=['last_activity'])\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "df_user_features.head()"
   ],
   "id": "7d6ea89f3efd3dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nombre de catégories à garder\n",
    "TOP_N = 20\n",
    "\n",
    "# 1. Identifier les TOP_N catégories achetées\n",
    "top_categories = (\n",
    "    df[df['event_type_purchase'] == 1]\n",
    "    .groupby('category_id')\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(TOP_N)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# 2. Créer un pivot avec achats uniquement, autres regroupées\n",
    "purchase_pivot = (\n",
    "    df[df['event_type_purchase'] == 1]\n",
    "    .assign(category_id=lambda x: x['category_id'].where(x['category_id'].isin(top_categories), 'other'))\n",
    "    .groupby(['user_id', 'category_id'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3. Ajouter le préfixe \"cat_\" sauf pour 'user_id'\n",
    "purchase_pivot.columns = [\n",
    "    'user_id' if col == 'user_id' else f\"cat_{col}\" for col in purchase_pivot.columns\n",
    "]\n",
    "\n",
    "# 4. Merge avec ton dataframe final\n",
    "df_final = df_user_features.merge(purchase_pivot, on='user_id', how='left').fillna(0)\n",
    "\n",
    "# Optionnel : normaliser en pourcentage\n",
    "category_cols = [col for col in purchase_pivot.columns if col != 'user_id']\n",
    "df_final[category_cols] = df_final[category_cols].div(\n",
    "    df_final[category_cols].sum(axis=1).replace(0, np.nan),\n",
    "    axis=0\n",
    ").fillna(0)\n",
    "\n",
    "print(df_final.head())"
   ],
   "id": "b0b9a89b72fa1c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cherchons maintenant les corrélations entre les différentes features",
   "id": "d454646cb1954bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_matrix = df_final.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7611c6369243303f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regrouper les clients par intéractions panier/nombre de connexions\n",
    "Achat par heure de la journée\n",
    "PySpark\n",
    "Analyses Bivariées\n",
    "Retravailler les données\n",
    "Qualitatif -> Quantitatif\n",
    "Test de Anova et Chi^2"
   ],
   "id": "28346a7764df4dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final = df_final.drop(['user_id'], axis=1)",
   "id": "44dc638bf51935f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final.describe()",
   "id": "6c04d37063a93fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_final)"
   ],
   "id": "98f24c708285e683",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modèle",
   "id": "d8f9d1b54f6e6313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"Variance expliquée cumulée : {pca.explained_variance_ratio_.cumsum()}\")\n",
    "df_pca"
   ],
   "id": "303bb6832f7be20c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'PC1': df_pca[:, 0],\n",
    "    'PC2': df_pca[:, 1],\n",
    "    'PC3': df_pca[:, 2],\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(df_plot, x='PC1', y='PC2', z='PC3',\n",
    "                    title='Visualisation PCA',\n",
    "                    opacity=0.7,\n",
    "                    size_max=5)\n",
    "\n",
    "fig.show()"
   ],
   "id": "fa1d1225ac89e202",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Recherche des hyperparamètres",
   "id": "77d850f85e9f3e07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "min_samples = df_pca.shape[1] + 1\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(df_pca)\n",
    "distances, indices = neighbors_fit.kneighbors(df_pca)\n",
    "\n",
    "# On prend la distance au dernier voisin (le k-ième)\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Tracer la courbe\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points triés\")\n",
    "plt.ylabel(f\"Distance au {min_samples}ème voisin\")\n",
    "plt.title(\"k-distance plot\")\n",
    "plt.show()"
   ],
   "id": "bcfc98265c6c685f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Min distance: {distances.min()}\")\n",
    "print(f\"Max distance: {distances.max()}\")\n",
    "print(f\"Moyenne distance: {distances.mean()}\")\n",
    "print(f\"25% quantile: {np.percentile(distances, 25)}\")\n",
    "print(f\"50% quantile: {np.percentile(distances, 50)}\")\n",
    "print(f\"75% quantile: {np.percentile(distances, 75)}\")"
   ],
   "id": "a0d6ee3b5129b40b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "sensitivities = [0.5, 1]\n",
    "eps_values = []\n",
    "\n",
    "for S in sensitivities:\n",
    "    kneedle = KneeLocator(range(len(distances)), distances, S=S, curve='convex', direction='increasing')\n",
    "    knee_idx = kneedle.knee\n",
    "    if knee_idx is not None:\n",
    "        eps = distances[knee_idx]\n",
    "        print(f\"eps détecté avec S={S} : {eps:.4f}\")\n",
    "        eps_values.append(eps)\n",
    "    else:\n",
    "        print(f\"Aucun coude détecté avec S={S}\")\n",
    "        eps_values.append(None)"
   ],
   "id": "568b21f626e97d87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "max_sample_size = 1000\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "for eps in eps_values:\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(df_pca)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_ratio = list(labels).count(-1) / len(labels)\n",
    "    print(f\"eps={eps:.2f} -> clusters: {n_clusters}, bruit: {noise_ratio*100:.3f}%\")\n",
    "\n",
    "    mask = labels != -1\n",
    "    X_no_noise = df_pca[mask]\n",
    "    labels_no_noise = labels[mask]\n",
    "\n",
    "    if n_clusters > 1 and len(X_no_noise) > 0:\n",
    "        sample_size = min(max_sample_size, len(X_no_noise))\n",
    "        indices = rng.choice(len(X_no_noise), sample_size, replace=False)\n",
    "        X_sample = X_no_noise[indices]\n",
    "        labels_sample = labels_no_noise[indices]\n",
    "\n",
    "        sil_score = silhouette_score(X_sample, labels_sample)\n",
    "        ch_score = calinski_harabasz_score(X_sample, labels_sample)\n",
    "        db_score = davies_bouldin_score(X_sample, labels_sample)\n",
    "\n",
    "        print(f\"  Silhouette Score    : {sil_score:.3f}\")\n",
    "        print(f\"  Calinski-Harabasz   : {ch_score:.3f}\")\n",
    "        print(f\"  Davies-Bouldin      : {db_score:.3f}\")\n",
    "\n",
    "\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            best_eps = eps\n",
    "\n",
    "    else:\n",
    "        print(\"Pas assez de clusters pour calculer les scores.\")"
   ],
   "id": "28b064b50a77c15f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modèle final",
   "id": "b158c7e2e5fb551d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dbscan = DBSCAN(eps=best_eps, min_samples=min_samples)\n",
    "clusters = dbscan.fit_predict(df_pca)"
   ],
   "id": "f6405a536b5d377e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "148f9e3d4cf8da1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisation des clusters",
   "id": "7959e68bb40b64e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'PC1': df_pca[:, 0],\n",
    "    'PC2': df_pca[:, 1],\n",
    "    'PC3': df_pca[:, 2],\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "df_plot['color'] = df_plot['cluster'].apply(lambda x: 'grey' if x == -1 else f'cluster {x}')\n",
    "\n",
    "fig = px.scatter_3d(df_plot, x='PC1', y='PC2', z='PC3',\n",
    "                    color='color',\n",
    "                    title='Clusters DBSCAN',\n",
    "                    opacity=0.7,\n",
    "                    size_max=5,\n",
    "                    labels={'color': 'Cluster'})\n",
    "\n",
    "fig.show()"
   ],
   "id": "70d70d92518e4bf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importance de chaque variable par cluster",
   "id": "4cba524d8b3aee6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Suppose que df_final est ton DataFrame des features\n",
    "# labels = clustering.labels_  # Sortie de DBSCAN\n",
    "\n",
    "df_clusters = df_final.copy()\n",
    "df_clusters['cluster'] = labels\n",
    "\n",
    "# On exclut le bruit (-1)\n",
    "df_clusters = df_clusters[df_clusters['cluster'] != -1]\n",
    "\n",
    "# Moyenne par cluster\n",
    "cluster_means = df_clusters.groupby('cluster').mean()\n",
    "\n",
    "# Moyenne globale\n",
    "global_mean = df_final.mean()\n",
    "\n",
    "# Importance relative : écart à la moyenne globale\n",
    "feature_importance = cluster_means - global_mean\n",
    "\n",
    "# Pour chaque cluster, trier les features par importance absolue\n",
    "for c in feature_importance.index:\n",
    "    print(f\"\\nCluster {c}\")\n",
    "    print(feature_importance.loc[c].abs().sort_values(ascending=False).head(10))"
   ],
   "id": "e8b3e261158b567",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
