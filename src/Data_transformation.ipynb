{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Définir ta configuration\n",
    "connection_parameters = {\n",
    "    \"account\": \"RSYIXFD-HT53341.snowflakecomputing.com\",   # ex: \"xy12345.eu-central-1\"\n",
    "    \"user\": \"<your-username>\",\n",
    "    \"password\": \"<your-password>\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "    \"database\": \"RAW_DATA\",\n",
    "    \"schema\": \"AMAZING_DATA\"\n",
    "}\n",
    "\n",
    "# Créer la session\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee26421-acec-4a13-bf3a-3f337f92a83c",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "df = session.table(\"RAW_DATA.AMAZING_DATA.CLIENT_EVENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a40a84-6792-4451-93de-cdb16b318153",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d3d87-2d5f-4c34-8cdf-2534551f798f",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.types import IntegerType\n",
    "import numpy as np\n",
    "\n",
    "# 1. Conversion du timestamp\n",
    "df = df.with_column(\"timestamp\", F.to_timestamp(F.col(\"EVENT_TIME\")))\n",
    "\n",
    "# 2. Extraction des composantes temporelles\n",
    "df = (\n",
    "    df.with_column(\"hour\", F.hour(\"timestamp\"))\n",
    "      .with_column(\"day\", F.dayofweek(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# 3. Catégorisation des moments de la journée - VERSION OPTIMISÉE SQL\n",
    "df = df.with_column(\n",
    "    \"time_period\",\n",
    "    F.when((F.col(\"hour\") >= 6) & (F.col(\"hour\") < 12), F.lit(\"morning\"))\n",
    "     .when((F.col(\"hour\") >= 12) & (F.col(\"hour\") < 18), F.lit(\"afternoon\"))  \n",
    "     .when((F.col(\"hour\") >= 18) & (F.col(\"hour\") < 24), F.lit(\"evening\"))\n",
    "     .otherwise(F.lit(\"night\"))\n",
    ")\n",
    "\n",
    "# 4. Encodage one-hot des types d'événements\n",
    "df = (\n",
    "    df.with_column(\"EVENT_TYPE_view\", F.when(F.col(\"EVENT_TYPE\")==\"view\", 1).otherwise(0))\n",
    "      .with_column(\"EVENT_TYPE_cart\", F.when(F.col(\"EVENT_TYPE\")==\"cart\", 1).otherwise(0))\n",
    "      .with_column(\"EVENT_TYPE_purchase\", F.when(F.col(\"EVENT_TYPE\")==\"purchase\", 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# 5. Agrégats principaux\n",
    "agg_behaviour = (\n",
    "    df.group_by(\"USER_ID\")\n",
    "      .agg(\n",
    "          F.sum(\"EVENT_TYPE_view\").alias(\"total_views\"),\n",
    "          F.sum(\"EVENT_TYPE_cart\").alias(\"total_cart\"),\n",
    "          F.sum(\"EVENT_TYPE_purchase\").alias(\"total_purchase\"),\n",
    "          F.count_distinct(\"CATEGORY_ID\").alias(\"unique_categories\"),\n",
    "          F.max(\"timestamp\").alias(\"last_activity\"),\n",
    "          F.count(\"*\").alias(\"total_events\")  # Ajouté ici pour éviter un recalcul\n",
    "      )\n",
    ")\n",
    "\n",
    "# 6. Total dépensé sur les achats\n",
    "total_spent = (\n",
    "    df.filter(F.col(\"EVENT_TYPE_purchase\") == 1)\n",
    "      .group_by(\"USER_ID\")\n",
    "      .agg(F.round(F.sum(\"PRICE\"), 2).alias(\"total_spent\"))\n",
    ")\n",
    "\n",
    "agg_behaviour = (\n",
    "    agg_behaviour.join(total_spent, on=\"USER_ID\", how=\"left\")\n",
    "                 .with_column(\"total_spent\", F.coalesce(F.col(\"total_spent\"), F.lit(0)))\n",
    ")\n",
    "\n",
    "# 7. KPIs dérivés\n",
    "agg_behaviour = (\n",
    "    agg_behaviour\n",
    "    .with_column(\n",
    "        \"avg_basket\",\n",
    "        F.when(F.col(\"total_purchase\") == 0, F.lit(0.0))\n",
    "         .otherwise(F.round(F.col(\"total_spent\") / F.col(\"total_purchase\"), 2))\n",
    "    )\n",
    "    .with_column(\n",
    "        \"conversion_rate\",\n",
    "        F.when(F.col(\"total_views\") == 0, F.lit(0.0))\n",
    "         .otherwise(F.round(F.col(\"total_purchase\") / F.col(\"total_views\"), 4))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 8. Répartition par moments de la journée - VERSION ULTRA OPTIMISÉE\n",
    "# Une seule requête pour tout calculer\n",
    "time_ratios = (\n",
    "    df.group_by(\"USER_ID\")\n",
    "      .agg(\n",
    "          # Compter directement chaque période\n",
    "          F.sum(F.when(F.col(\"time_period\") == \"morning\", 1).otherwise(0)).alias(\"morning_count\"),\n",
    "          F.sum(F.when(F.col(\"time_period\") == \"afternoon\", 1).otherwise(0)).alias(\"afternoon_count\"),\n",
    "          F.sum(F.when(F.col(\"time_period\") == \"evening\", 1).otherwise(0)).alias(\"evening_count\"),\n",
    "          F.sum(F.when(F.col(\"time_period\") == \"night\", 1).otherwise(0)).alias(\"night_count\"),\n",
    "          F.count(\"*\").alias(\"total_user_events\")\n",
    "      )\n",
    "      .with_column(\"morning\", F.round(F.col(\"morning_count\") / F.col(\"total_user_events\"), 4))\n",
    "      .with_column(\"afternoon\", F.round(F.col(\"afternoon_count\") / F.col(\"total_user_events\"), 4))\n",
    "      .with_column(\"evening\", F.round(F.col(\"evening_count\") / F.col(\"total_user_events\"), 4))\n",
    "      .with_column(\"night\", F.round(F.col(\"night_count\") / F.col(\"total_user_events\"), 4))\n",
    "      .select(\"USER_ID\", \"morning\", \"afternoon\", \"evening\", \"night\")\n",
    ")\n",
    "\n",
    "# 9. Heure moyenne d'activité - VERSION OPTIMISÉE\n",
    "hour_stats = (\n",
    "    df.group_by(\"USER_ID\")\n",
    "      .agg(\n",
    "          F.avg(F.cos(F.lit(2 * np.pi) * F.col(\"hour\") / F.lit(24))).alias(\"hour_cos\"),\n",
    "          F.avg(F.sin(F.lit(2 * np.pi) * F.col(\"hour\") / F.lit(24))).alias(\"hour_sin\")\n",
    "      )\n",
    "      .with_column(\n",
    "          \"peak_hour_rad\", \n",
    "          F.atan2(F.col(\"hour_sin\"), F.col(\"hour_cos\"))\n",
    "      )\n",
    "      .with_column(\n",
    "          \"peak_hour\",\n",
    "          F.round(\n",
    "              F.when(\n",
    "                  F.col(\"peak_hour_rad\") < 0,\n",
    "                  (F.col(\"peak_hour_rad\") + F.lit(2 * np.pi)) * F.lit(24) / F.lit(2 * np.pi)\n",
    "              ).otherwise(\n",
    "                  F.col(\"peak_hour_rad\") * F.lit(24) / F.lit(2 * np.pi)\n",
    "              ),\n",
    "              2\n",
    "          )\n",
    "      )\n",
    "      .select(\"USER_ID\", \"peak_hour\")\n",
    ")\n",
    "\n",
    "# 10. Récence en jours\n",
    "now = df.agg(F.max(\"timestamp\").alias(\"max_ts\")).collect()[0][\"MAX_TS\"]\n",
    "agg_behaviour = agg_behaviour.with_column(\n",
    "    \"recency_days\",\n",
    "    F.datediff(\"day\", F.col(\"last_activity\"), F.lit(now))\n",
    ")\n",
    "\n",
    "# 11. Fusion finale - UNE SEULE JOINTURE\n",
    "df_user_features = (\n",
    "    agg_behaviour.join(time_ratios, on=\"USER_ID\", how=\"left\")\n",
    "                 .join(hour_stats, on=\"USER_ID\", how=\"left\")\n",
    "                 .drop(\"last_activity\", \"total_events\")  # Nettoyage\n",
    ")\n",
    "\n",
    "# 12. Remplacer les nulls (si il y en a)\n",
    "numeric_columns = [\"total_views\", \"total_cart\", \"total_purchase\", \"unique_categories\", \n",
    "                  \"total_spent\", \"avg_basket\", \"conversion_rate\", \"recency_days\",\n",
    "                  \"morning\", \"afternoon\", \"evening\", \"night\", \"peak_hour\"]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df_user_features.columns:\n",
    "        df_user_features = df_user_features.with_column(\n",
    "            col, \n",
    "            F.coalesce(F.col(col), F.lit(0.0))\n",
    "        )\n",
    "\n",
    "df_user_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa133e6f-d977-466f-8a5a-0da7cebb3379",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "TOP_N = 20\n",
    "\n",
    "# 1) Identifier les top 20 catégories et créer les features en une seule fois\n",
    "top_categories_list = [\n",
    "    row[\"CATEGORY_ID\"] for row in (\n",
    "        df.filter(F.col(\"EVENT_TYPE_purchase\") == 1)\n",
    "          .group_by(\"CATEGORY_ID\")\n",
    "          .agg(F.count(\"*\").alias(\"purchase_count\"))\n",
    "          .sort(F.desc(\"purchase_count\"))\n",
    "          .limit(TOP_N)\n",
    "          .collect()\n",
    "    ) if row[\"CATEGORY_ID\"] is not None\n",
    "]\n",
    "\n",
    "# 2) Calculer directement les pourcentages par catégorie - VERSION ULTRA OPTIMISÉE\n",
    "# Une seule requête pour tout calculer\n",
    "category_features = (\n",
    "    df.filter(F.col(\"EVENT_TYPE_purchase\") == 1)\n",
    "      .group_by(\"USER_ID\")\n",
    "      .agg([\n",
    "          # Calculer directement le ratio de chaque top catégorie\n",
    "          *[F.round(\n",
    "              F.sum(F.when(F.col(\"CATEGORY_ID\") == cat, 1).otherwise(0)) / F.count(\"*\"), \n",
    "              4\n",
    "          ).alias(f\"cat_{str(cat).replace('-', '_').replace(' ', '_').replace('.', '_')}\")\n",
    "          for cat in top_categories_list],\n",
    "          # Calculer le ratio des \"autres\" catégories\n",
    "          F.round(\n",
    "              F.sum(F.when(~F.col(\"CATEGORY_ID\").isin(top_categories_list), 1).otherwise(0)) / F.count(\"*\"),\n",
    "              4\n",
    "          ).alias(\"cat_others\")\n",
    "      ])\n",
    ")\n",
    "\n",
    "# 3) Nettoyer les noms de colonnes qui commencent par un chiffre\n",
    "final_columns = [\"USER_ID\"]\n",
    "for col in category_features.columns[1:]:  # Skip USER_ID\n",
    "    if col.startswith(\"cat_\") and len(col) > 4:\n",
    "        col_suffix = col[4:]  # Enlever \"cat_\"\n",
    "        if col_suffix[0].isdigit():\n",
    "            new_name = f\"cat_c_{col_suffix}\"\n",
    "            category_features = category_features.with_column_renamed(col, new_name)\n",
    "            final_columns.append(new_name)\n",
    "        else:\n",
    "            final_columns.append(col)\n",
    "    else:\n",
    "        final_columns.append(col)\n",
    "\n",
    "# 4) Jointure finale avec gestion des nulls\n",
    "df_final = df_user_features.join(category_features, on=\"USER_ID\", how=\"left\")\n",
    "\n",
    "# Remplacer les nulls par 0 pour les utilisateurs sans achats\n",
    "category_cols = [col for col in df_final.columns if col.startswith(\"cat_\")]\n",
    "\n",
    "# Si pas de colonnes cat_ trouvées, chercher autrement\n",
    "if not category_cols:\n",
    "    category_cols = [col for col in category_features.columns if col != \"USER_ID\"]\n",
    "\n",
    "# Remplacer les nulls\n",
    "for col_name in category_cols:\n",
    "    df_final = df_final.with_column(\n",
    "        col_name, \n",
    "        F.round(F.coalesce(F.col(col_name), F.lit(0.0)), 4)\n",
    "    )\n",
    "\n",
    "# Affichage final\n",
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a31ce5-4972-41e5-8708-59419acc18cb",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# from snowflake.snowpark import functions as F\n",
    "\n",
    "# # 1) Identifier les colonnes numériques à standardiser\n",
    "# numeric_columns = [\"TOTAL_VIEWS\", \"TOTAL_CART\", \"TOTAL_PURCHASE\", \"UNIQUE_CATEGORIES\", \n",
    "#                   \"TOTAL_SPENT\", \"AVG_BASKET\", \"RECENCY_DAYS\", \"PEAK_HOUR\"]\n",
    "\n",
    "# # 2) Calculer mean et std pour toutes les colonnes en une seule requête\n",
    "# stats_expressions = []\n",
    "# for col in numeric_columns:\n",
    "#     stats_expressions.extend([\n",
    "#         F.avg(col).alias(f\"{col}_mean\"),\n",
    "#         F.stddev(col).alias(f\"{col}_std\")\n",
    "#     ])\n",
    "\n",
    "# # Une seule requête pour calculer toutes les stats\n",
    "# stats_row = df_final.select(*stats_expressions).collect()[0]\n",
    "\n",
    "# # 3) Créer le DataFrame standardisé en une seule fois\n",
    "# standardized_expressions = [F.col(\"USER_ID\")]  # Garder l'ID\n",
    "\n",
    "# # Construire la liste des noms de colonnes standardisées pour la vérification\n",
    "# scaled_column_names = []\n",
    "\n",
    "# for col in numeric_columns:\n",
    "#     mean_val = stats_row[f\"{col}_MEAN\"] or 0.0\n",
    "#     std_val = stats_row[f\"{col}_STD\"] or 1.0\n",
    "    \n",
    "#     # Éviter division par zéro\n",
    "#     if std_val == 0:\n",
    "#         std_val = 1.0\n",
    "    \n",
    "#     # Expression de standardisation: (x - mean) / std\n",
    "#     scaled_col_name = f\"{col}_scaled\"\n",
    "#     scaled_column_names.append(scaled_col_name)\n",
    "    \n",
    "#     standardized_expr = F.round(\n",
    "#         (F.col(col) - F.lit(mean_val)) / F.lit(std_val),\n",
    "#         4\n",
    "#     ).alias(scaled_col_name)\n",
    "    \n",
    "#     standardized_expressions.append(standardized_expr)\n",
    "\n",
    "# # 4) Récupérer les autres colonnes non-numériques (si nécessaire)\n",
    "# try:\n",
    "#     all_columns = df_final.columns\n",
    "#     other_columns = [col for col in all_columns \n",
    "#                     if col not in numeric_columns and col != \"USER_ID\"]\n",
    "    \n",
    "#     # Ajouter les colonnes non-numériques\n",
    "#     for col in other_columns:\n",
    "#         standardized_expressions.append(F.col(col))\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"Note: Impossible de récupérer toutes les colonnes: {e}\")\n",
    "#     other_columns = []\n",
    "\n",
    "# # 5) Créer le DataFrame standardisé\n",
    "# df_standardized = df_final.select(*standardized_expressions)\n",
    "\n",
    "# # Prendre les 3 premières colonnes standardisées\n",
    "# sample_cols = scaled_column_names[:3]\n",
    "# verification_expr = []\n",
    "\n",
    "# for col in sample_cols:\n",
    "#     verification_expr.extend([\n",
    "#         F.round(F.avg(col), 4).alias(f\"{col}_mean\"),\n",
    "#         F.round(F.stddev(col), 4).alias(f\"{col}_std\")\n",
    "#     ])\n",
    "\n",
    "# df_standardized.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cde82-c626-4ac9-ada8-a43c882de4a6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# Sauvegarde dans une nouvelle table\n",
    "df_final.write.save_as_table(\n",
    "    \"PROCESSED_DATA.AMAZING_DATA.CLIENT_EVENTS_CLEAN\",\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "tristanrib_@outlook.fr",
   "authorId": "2113048723087",
   "authorName": "TRISTANRIB",
   "lastEditTime": 1756032382373,
   "notebookId": "xhns5jgenriswmbvj7nw",
   "sessionId": "caa7d08f-fe68-4b4d-b907-f8dea6c49cf1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
