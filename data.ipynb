{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regrouper les clients par intéractions panier/nombre de connexions\n",
    "Achat par heure de la journée\n",
    "PySpark\n",
    "Analyses Bivariées\n",
    "Retravailler les données\n",
    "Qualitatif -> Quantitatif\n",
    "Test de Anova et Chi^2"
   ],
   "id": "598d4edaa02a777a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "id": "731c85086afbb34e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_to_load = \"./DATA/2019-Oct.csv\"\n",
    "\n",
    "chunksize = 50_000\n",
    "fraction = 0.01\n",
    "seed = 42"
   ],
   "id": "906c81ca65d6d270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunks = pd.read_csv(file_to_load, chunksize=chunksize, parse_dates=['event_time'])",
   "id": "302d110db2b8404d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def getRandomDataset(chunks, frac, seed):\n",
    "    return pd.concat(chunk.sample(frac=frac, random_state=seed) for chunk in chunks)"
   ],
   "id": "fab71c68bf1d6d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = getRandomDataset(chunks, fraction, seed)",
   "id": "71f417833fb1718b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploration",
   "id": "4fe5ac4a7eef382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "c0f321cd889d671c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "a863ac1e53a6a3e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "97bfc2b095e7948c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def createNaAndUniqueMatrix(df):\n",
    "    rows = [\"Valeurs nulles\", \"Valeurs uniques\"]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for column in df.columns:    \n",
    "        na = df[column].isna()\n",
    "        unique = len(df[column][~na].unique())\n",
    "\n",
    "        data[column] = [f'{len(df[na])} ({(len(df[na])/len(df))*100:.2f}%)', unique]\n",
    "\n",
    "    return pd.DataFrame(data, index=rows)"
   ],
   "id": "58a099deffaca8ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "createNaAndUniqueMatrix(df)",
   "id": "e1f7b730b9a1f71e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analysons les 3 valeurs uniques d'event_type",
   "id": "2f77b2a0fd3cc283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frequences = df[\"event_type\"][df[\"event_type\"].notna()].value_counts()\n",
    "\n",
    "ax = frequences.plot(kind=\"bar\")\n",
    "\n",
    "for i,v in enumerate(frequences):\n",
    "    ax.text(i, v + 0.1, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Fréquence des types d'événements\")\n",
    "plt.xlabel(\"Événement\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()"
   ],
   "id": "e67341e753c2a153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On recense donc 407 787 vues, 9222 mises dans un panier et 7483 achats",
   "id": "c7806ab6a2207f68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quels sont les taux de conversion entre les types d'événements ?",
   "id": "c96ab6355617e0b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "viewToCart = frequences[\"cart\"] / frequences[\"view\"] * 100\n",
    "cartToPurchase = frequences[\"purchase\"] / frequences[\"cart\"] * 100\n",
    "viewToPurchase = frequences[\"purchase\"] / frequences[\"view\"] * 100\n",
    "\n",
    "print(f\"Taux de conversion vue => panier : {viewToCart:.2f} %\")\n",
    "print(f\"Taux de conversion panier => achat : {cartToPurchase:.2f} %\")\n",
    "print(f\"Taux de conversion vue => achat : {viewToPurchase:.2f} %\")"
   ],
   "id": "c14b5463237a7641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combien d'utilisateurs différents ont fait un achat ?",
   "id": "dbe89ac1e56137fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "usersPurchased = len(df[\"user_id\"][df[\"event_type\"] == \"purchase\"].unique())\n",
    "\n",
    "itemsPurchasedPerUser = frequences[\"purchase\"] / usersPurchased\n",
    "\n",
    "maxUserBuy = df[df[\"event_type\"] == \"purchase\"].groupby(\"user_id\").size().max()\n",
    "maxSessionBuy = df[df[\"event_type\"] == \"purchase\"].groupby(\"user_session\").size().max()\n",
    "\n",
    "print(f\"Un total de {usersPurchased} utilisateurs différents ont effectué des achats sur le site\")\n",
    "print(f\"Un utilisateur qui achète sur le site achète en moyenne {itemsPurchasedPerUser:.2f} articles\")\n",
    "print(f\"L'utilisateur qui a le plus acheté a acheté {maxUserBuy} articles sur le mois\")\n",
    "print(f\"La session utilisateur qui a le plus acheté a acheté {maxSessionBuy} articles en une session\")"
   ],
   "id": "f587aa8d13692513",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comment évoluent les achats sur le mois ?",
   "id": "9aab89df95fa5017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datesDayToDay = pd.to_datetime(df[\"event_time\"]).dt.date\n",
    "purchasesDayToDay = df[df[\"event_type\"] == \"purchase\"].groupby(datesDayToDay)[\"event_type\"].count()\n",
    "uniqueDates = datesDayToDay.unique()"
   ],
   "id": "d10c8261c2337eeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(uniqueDates, purchasesDayToDay)\n",
    "\n",
    "for x,y in zip(uniqueDates, purchasesDayToDay):\n",
    "    plt.text(x, y + 0.3, y, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.title(\"Ventes effectués au cours du mois\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Quantité\")\n",
    "plt.grid(True)\n",
    "plt.xticks(uniqueDates, rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "id": "611f7d6c26e6d63b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quel est le chiffre d'affaire quotidien ?",
   "id": "15adde2d34dc672d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datesDayToDay = pd.to_datetime(df[\"event_time\"]).dt.date\n",
    "gainsDayToDay = df[df[\"event_type\"] == \"purchase\"].groupby(datesDayToDay)[\"price\"].sum()\n",
    "uniqueDates = datesDayToDay.unique()"
   ],
   "id": "a25528983679c28c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.bar(uniqueDates, gainsDayToDay)\n",
    "\n",
    "for x,y in zip(uniqueDates, gainsDayToDay):\n",
    "    plt.text(x, y + 0.3, y, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.title(\"Chiffre d'affaire effectué au cours du mois\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Chiffre d'affaire\")\n",
    "plt.grid(True)\n",
    "plt.xticks(uniqueDates, rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "id": "70366d571a505fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformation",
   "id": "df080d58bd327c1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "9b6b031b9402ae75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.get_dummies(df, columns=[\"event_type\"])",
   "id": "106a12ada57d35bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "encoder = OneHotEncoder(sparse_output=False)",
   "id": "cd0a4a0424d66d29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Encoder chaque cycle en cos/sin\n",
    "df['day_of_year'] = df['event_time'].dt.dayofyear\n",
    "df['day_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['day_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "df['day_of_week'] = df['event_time'].dt.weekday  # 0=lundi\n",
    "df['day_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "df['hour'] = df['event_time'].dt.hour\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "df['minute'] = df['event_time'].dt.hour * 60 + df['event_time'].dt.minute\n",
    "df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 1440)\n",
    "df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 1440)\n",
    "\n",
    "df.drop(['event_time', 'day_of_year', 'day_of_week', 'hour', 'minute'], axis=1, inplace=True)"
   ],
   "id": "14e66cd1425b4132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "f77a663058d3c6dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7a4615eeef069ffa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final = df.drop(['product_id', 'category_id', 'category_code', 'brand', 'user_session'], axis=1)",
   "id": "50986d7515c0253a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_final)"
   ],
   "id": "dbff6ca4105d7be6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_scaled",
   "id": "65407b0b3957d175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modèle",
   "id": "3ff8feb2b643a9f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"Variance expliquée cumulée : {pca.explained_variance_ratio_.cumsum()}\")"
   ],
   "id": "2c522cc5104a584e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'PC1': df_pca[:, 0],\n",
    "    'PC2': df_pca[:, 1],\n",
    "    'PC3': df_pca[:, 2],\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(df_plot, x='PC1', y='PC2', z='PC3',\n",
    "                    title='Visualisation PCA',\n",
    "                    opacity=0.7,\n",
    "                    size_max=5)\n",
    "\n",
    "fig.show()"
   ],
   "id": "8ff5ac5ed25cf73d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Recherche des hyperparamètres",
   "id": "882dbafc85326c54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "min_samples = df_pca.shape[1] + 1\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(df_pca)\n",
    "distances, indices = neighbors_fit.kneighbors(df_pca)\n",
    "\n",
    "# On prend la distance au dernier voisin (le k-ième)\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Tracer la courbe\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points triés\")\n",
    "plt.ylabel(f\"Distance au {min_samples}ème voisin\")\n",
    "plt.title(\"k-distance plot\")\n",
    "plt.show()"
   ],
   "id": "64f8114ceb20f7b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Min distance: {distances.min()}\")\n",
    "print(f\"Max distance: {distances.max()}\")\n",
    "print(f\"Moyenne distance: {distances.mean()}\")\n",
    "print(f\"25% quantile: {np.percentile(distances, 25)}\")\n",
    "print(f\"50% quantile: {np.percentile(distances, 50)}\")\n",
    "print(f\"75% quantile: {np.percentile(distances, 75)}\")"
   ],
   "id": "7d168f1a5023197d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "sensitivities = [0.5, 1]\n",
    "eps_values = []\n",
    "\n",
    "for S in sensitivities:\n",
    "    kneedle = KneeLocator(range(len(distances)), distances, S=S, curve='convex', direction='increasing')\n",
    "    knee_idx = kneedle.knee\n",
    "    if knee_idx is not None:\n",
    "        eps = distances[knee_idx]\n",
    "        print(f\"eps détecté avec S={S} : {eps:.4f}\")\n",
    "        eps_values.append(eps)\n",
    "    else:\n",
    "        print(f\"Aucun coude détecté avec S={S}\")\n",
    "        eps_values.append(None)"
   ],
   "id": "cf8c1913e46291f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "max_sample_size = 10000\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(df_pca)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_ratio = list(labels).count(-1) / len(labels)\n",
    "    print(f\"eps={eps:.2f} -> clusters: {n_clusters}, bruit: {noise_ratio*100:.3f}%\")\n",
    "\n",
    "    mask = labels != -1\n",
    "    X_no_noise = df_pca[mask]\n",
    "    labels_no_noise = labels[mask]\n",
    "\n",
    "    if n_clusters > 1 and len(X_no_noise) > 0:\n",
    "        sample_size = min(max_sample_size, len(X_no_noise))\n",
    "        indices = np.random.choice(len(X_no_noise), sample_size, replace=False)\n",
    "        X_sample = X_no_noise[indices]\n",
    "        labels_sample = labels_no_noise[indices]\n",
    "\n",
    "        sil_score = silhouette_score(X_sample, labels_sample)\n",
    "        ch_score = calinski_harabasz_score(X_sample, labels_sample)\n",
    "        db_score = davies_bouldin_score(X_sample, labels_sample)\n",
    "\n",
    "        print(f\"  Silhouette Score    : {sil_score:.3f}\")\n",
    "        print(f\"  Calinski-Harabasz   : {ch_score:.3f}\")\n",
    "        print(f\"  Davies-Bouldin      : {db_score:.3f}\")\n",
    "\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            best_eps = eps\n",
    "    else:\n",
    "        print(\"Pas assez de clusters pour calculer les scores.\")"
   ],
   "id": "5329470dd43b5b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modèle final",
   "id": "1a89896c5fd3603c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.cluster import HDBSCAN\n",
    "#\n",
    "# for mcs in [10000, 50000, 10000]:\n",
    "#     hdb = HDBSCAN(min_cluster_size=mcs, min_samples=None)\n",
    "#     labels = hdb.fit_predict(df_pca)\n",
    "#     n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#     noise_pct = (list(labels).count(-1) / len(labels)) * 100\n",
    "#     print(f\"mcs={mcs} -> clusters: {n_clusters}, bruit: {noise_pct:.3f}%\")"
   ],
   "id": "125acb0fc314c7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dbscan = DBSCAN(eps=best_eps, min_samples=min_samples)\n",
    "clusters = dbscan.fit_predict(df_pca)"
   ],
   "id": "71b93c62d5465a5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisation des clusters",
   "id": "f7c6b2116a9b76eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'PC1': df_pca[:, 0],\n",
    "    'PC2': df_pca[:, 1],\n",
    "    'PC3': df_pca[:, 2],\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "df_plot['color'] = df_plot['cluster'].apply(lambda x: 'grey' if x == -1 else f'cluster {x}')\n",
    "\n",
    "fig = px.scatter_3d(df_plot, x='PC1', y='PC2', z='PC3',\n",
    "                    color='color',\n",
    "                    title='Clusters DBSCAN',\n",
    "                    opacity=0.7,\n",
    "                    size_max=5,\n",
    "                    labels={'color': 'Cluster'})\n",
    "\n",
    "fig.show()"
   ],
   "id": "585f0d1f2cf8dbb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "d47422236db6aebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mask = clusters != -1\n",
    "#\n",
    "# print(f\"Silhouette Score    : {silhouette_score(df_pca[mask], clusters[mask]):.3f}\")\n",
    "# print(f\"Calinski-Harabasz   : {calinski_harabasz_score(df_pca[mask], clusters[mask]):.3f}\")\n",
    "# print(f\"Davies-Bouldin      : {davies_bouldin_score(df_pca[mask], clusters[mask]):.3f}\")"
   ],
   "id": "e3680402ade44fc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
